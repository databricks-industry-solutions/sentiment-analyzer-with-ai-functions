{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35e27579-321e-459f-ad90-96d34cb5dda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"https://cme-solution-accelerators-images.s3-us-west-2.amazonaws.com/toxicity/solution-accelerator-logo.png\"; width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e03061-0c79-42af-b1f9-8c72bd825d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# About This Series of Notebooks\n",
    "\n",
    "This series of notebooks is intended to help you use AI_Query functions in Databricks and identify common sentiment and features from your customer feedback\n",
    "\n",
    "In support of this goal, we will:\n",
    "\n",
    "- Load customer feedback data from Amazon\n",
    "- Use out of the box [AI functions](https://docs.databricks.com/aws/en/large-language-models/ai-functions) in Databricks to deliver batch inference sentiment analysis on your data in only a few lines of code\n",
    "- Create  a single, simple pipeline to detect sentiment. This pipeline can then be used for managing tables for reporting, ad hoc queries, and/or decision support.\n",
    "- Create a Genie room so you can explore your sentiment data with natural language interactions \n",
    "- Create a dashboard for monitoring sentiment back to the business and drive insights and action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "051498c7-662b-4b0c-aa6b-c59ec391a7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "In the previous, the review data was successfully downloaded, ingested into Databricks and stored in a table in Unity Catalog.\n",
    "\n",
    "In this notebook, we'll cover the basics of using AI_QUERY() to help us understand the process, before moving to apply this as a batch job across our entire dataset.\n",
    "\n",
    "The end result will be a new table containing our AI outputs which have been parsed and configured for downstream analysis by our analytical teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9d533d-f7ca-4973-b7c8-95c34d155a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.text(\"catalog\",'mido_edw_dev')\n",
    "schema = dbutils.widgets.text(\"schema\",'sentiment_analysis')\n",
    "volume = dbutils.widgets.text(\"volume\",'reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649956d7-3f00-444f-9fb9-1151578512ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Config"
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "volume = dbutils.widgets.get(\"volume\")\n",
    "transpiledtable = 'amazon_reviews_sentiment'\n",
    "transpiledtable = f'{catalog}.{schema}.{transpiledtable}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96fe669f-b8a8-4e2d-aad5-01a7d51de293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Inference\n",
    "\n",
    "This next step is critical to the process. For the sake of clarity, and keeping with the AI theme, Inference is defined as:\n",
    "\n",
    "_...process of using a trained machine learning model to make predictions or generate outputs based on new, unseen data. It’s the “deployment” phase of an ML model — where the model is no longer learning, but applying what it has already learned._\n",
    "\n",
    "_In Databricks, batch inference refers to running predictions on large datasets at rest (as opposed to streaming), often using tools like AI_QUERY to apply models at scale directly within SQL workflows._\n",
    "\n",
    "In this specific scenario, we'll be performing said batch inference against our customer sentinement data, by passing a prompt (instruction) combined with our data, in order to get outputs we can then use and act upon.\n",
    "\n",
    "Our first step will be to define the prompt that will be sent to the foundational model to be used by AI_Query to score our sentiment.\n",
    "\n",
    "The prompt can be customised as needed for the use case, with the example below designed to provide a json-friendly output that can be subsequently processed and used in downstream analytics\n",
    "\n",
    "To begin with, we'll explore the AI_QUERY function for an individual query to demonstrate the functionality\n",
    "\n",
    "[AI_QUERY](https://learn.microsoft.com/en-us/azure/databricks/large-language-models/ai-query-batch-inference) works by passing a prompt to the AI_QUERY function that returns an output from a model hosted in or outside of Databricks.\n",
    "\n",
    "Users can leverage both [Databricks-hosted foundation models](https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis/) (this example) or bring their own custom or fine-tuned models\n",
    "\n",
    "***NOTE*** For a lighter touch approach to sentiment analysis that does not require a full prompt but still gives you the releent scoring, please check out the AI_Function https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_analyze_sentiment too, if you don't need to the level of customisation shown in this accelerator\n",
    "\n",
    "![](images/inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4659288a-aba3-4ac1-960d-448160b9d418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Single Row Use of AI_Query\n",
    "\n",
    "In our first example, we pass a simple, single prompt to a model to view the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38a5b4b-0c70-461b-b1cb-2c71ee6ea753",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple AI_Query Example"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This examples calls AI_QUERY using a single row prompt. We use the provided databricks-meta-llama-3-3-70b-instruct endpoint for inference.\n",
    "\n",
    "DECLARE llmprompt string;\n",
    "\n",
    "SET var llmprompt = \"Tell me why sentiment analysis is so important for product companies\";\n",
    "\n",
    "SELECT AI_QUERY('databricks-meta-llama-3-3-70b-instruct', llmprompt) as Output;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "851e8e9d-2f59-4533-8d95-2647a6ca241b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#How Much Can It Help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119b81e3-74a3-4ec9-9858-e0617a13c6a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This examples calls AI_QUERY using a single row prompt. We use the provided databricks-meta-llama-3-3-70b-instruct endpoint for inference.\n",
    "\n",
    "DECLARE llmpromptexample string;\n",
    "\n",
    "SET var llmpromptexample = \"What percent improvement/impact could such a sentiment analysis solution have for a product company?\";\n",
    "\n",
    "SELECT AI_QUERY('databricks-meta-llama-3-3-70b-instruct', llmprompt) as Output;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3923763d-2fc9-4188-93d5-612d6f674318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Inference\n",
    "\n",
    "With that example demonstrated, we can take this a step further with [Batch Inference](https://docs.databricks.com/aws/en/large-language-models/ai-query-batch-inference). This allows us to perform a similar analysis but at scale, leveraging the power of Databricks to perform high volume inference in a view lines of code.\n",
    "\n",
    "As before, we'll define a prompt. This time it will be much more detailed, allowing us to capture the necessary outputs in terms of sentiment, themes and suggestions that provide clear insights for our downstream teams to act upon.\n",
    "\n",
    "The output will be stored in a table, providing a performant, scalable source from which we can use our Genie room and AI/BI Dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf56b25-dbce-43b8-9263-1718ab2ade3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Prompt"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#advanced prompt\n",
    "prompt = \"\"\"\n",
    "You are an AI sentiment analysis assistant specializing in product feedback evaluation.  \n",
    "Given the following product reviews, perform the following tasks:  \n",
    "\n",
    "1. **Sentiment Analysis:**  \n",
    "   - Classify the sentiment as **Positive, Neutral, or Negative**.  \n",
    "   - Provide a **sentiment score** between **-1 (very negative) and +1 (very positive)**.  \n",
    "\n",
    "2. **Key Themes & Insights:**  \n",
    "   - Identify the **main topics** customers are mentioning (e.g., usability, features, pricing).  \n",
    "   - Summarize key takeaways based on the feedback provided.  \n",
    "\n",
    "3. **Improvement Suggestions:**  \n",
    "   - If there are concerns or negative feedback, suggest potential **improvements**.  \n",
    "\n",
    "4. **Highlight Customer Quotes:**  \n",
    "   - Extract **1-2 key quotes** that best represent the overall sentiment.  \n",
    "\n",
    "\n",
    "Give the response in a JSON format with the following fields:\n",
    "\n",
    "sentiment: Positive/Neutral/Negative,\n",
    "sentiment_score: FLOAT_VALUE,\n",
    "key_themes: [Theme 1, Theme 2, Theme 3],\n",
    "suggestions: Actionable recommendations for improvement.,\n",
    "highlight_quotes: [Quote 1, Quote 2]\n",
    "\n",
    "Do not include any text before the sentiment, sentiment score, key themes, suggestions, or highlight quotes, such as 'Here is the analysis of the product review in JSON format:'\n",
    "\n",
    "Do not include any back tics like ```json\n",
    "\n",
    "Ensure the key_themes are standardised across the returned dataset to allow for consistent analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11f35f4e-d721-40b3-a073-6929a47ed8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##Batch Inference with AI_Query\n",
    "\n",
    "With our prompt defined, we go to the critical step. Here, we do the following:\n",
    "\n",
    "- Pass our prompt to the AI_QUERY function, along with the the review data from our reviews table\n",
    "- Perform batch inference at scale, producing outputs as defined by the prompt instructions\n",
    "- Store the output in a Transpiled table for downstream analysis\n",
    "\n",
    "As a reminder, we can perform a quick query on our source table to view the relevant fields for our LLM to analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37f1d84-f0c5-4a2d-a74e-4e7b720273d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Query base table for example review and schema\n",
    "\n",
    "spark.sql(f\"select * from {catalog}.{schema}.amazon_reviews limit 10\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c67851-1444-40a9-8053-36cedd1861d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##Calling AI_QUERY\n",
    "\n",
    "We now call the AI_QUERY function by passing in our prompt and data. Note, in this example we concenate both the title and test together, as both have useful data to be used by the LLM.\n",
    "\n",
    "The output will be stored in the defined {transpiledtable}.\n",
    "\n",
    "Depending on your own dataset and models used, this next process may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d970de1-ba80-4b18-8b56-6cdd2ae9a8e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process Inference"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "#replace the endpoint with your own and remember to amend relevant table details\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {transpiledtable}\n",
    "as\n",
    "SELECT \n",
    "    parent_asin,\n",
    "    ai_query(\n",
    "    'databricks-meta-llama-3-3-70b-instruct',\n",
    "    \"{prompt}\"||concat(title,',', text)) AS output\n",
    "    FROM {catalog}.{schema}.amazon_reviews limit 10000\n",
    "    \"\"\"\n",
    "\n",
    "display(spark.sql(query))\n",
    "\n",
    "# 'databricks-meta-llama-3-3-70b-instruct',\n",
    "#meta-llama-3-1-405b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29b3fecb-3aac-4194-87d6-658df485b1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can now view the output from the inference by viewing the Transpiled table details, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17dff19-689b-4cb4-ba91-a727d30930ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Transpiled Data"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {transpiledtable}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4779a978-7a97-4d16-adc1-ed9eba54b538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Conclusion\n",
    "\n",
    "In this section we've used the AI_QUERY function to process our review data, using the prompt passed as defined earlier.\n",
    "\n",
    "The LLM output has been stored in a processed table that can now be used in the steps for our Genie Space and Dashboards.\n",
    "\n",
    "Now proceed to the final notebook, 003 Serving, to complete this accelerator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dda910ad-e74d-42d5-b3d9-d47d37542fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2841115366984018,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "002 Inference",
   "widgets": {
    "catalog": {
     "currentValue": "mido_edw_dev",
     "nuid": "a8cda791-aef4-4b53-9ad6-4a381ef7d017",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mido_edw_dev",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mido_edw_dev",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "sentiment_analysis",
     "nuid": "417fde35-d412-469d-ade4-5cfc0e2aa9d8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sentiment_analysis",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sentiment_analysis",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume": {
     "currentValue": "reviews",
     "nuid": "82fac45a-e1cb-4c07-ad47-84c365d79512",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "reviews",
      "label": null,
      "name": "volume",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "reviews",
      "label": null,
      "name": "volume",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
